# 实用资源链接

## 代码实现

### Flash-Attention-with-Bias-Triton

- 链接：[Flash-Attention-with-Bias-Triton](https://deepwiki.com/pengzhangzhi/Flash-Attention-with-Bias-Triton/4.1-attention_triton-function)
- 描述：使用 Triton 语言实现的高性能 Flash Attention 算法，支持矩阵形式的偏置张量
- 特点：
  - 为需要偏置项的 Transformer 模型提供高效实现
  - 在长序列长度场景下性能提升显著
  - 适合学习和研究注意力机制优化

## 计算资源

### Lambda GPU Cloud

- 链接：[Lambda GPU Cloud](https://lambda.ai/service/gpu-cloud)
- 描述：企业级云 GPU 服务平台
- 特点：
  - 快速部署
  - 支持 A100 (80GB) 等高性能 GPU
  - 价格参考：A100 约 13 元/小时
  - 适合实验和训练使用
